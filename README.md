# Calculator-Restful-Web-Service

# 1. tarsem 2309276
Title: Submillimeter Wave High Order Frequency Multiplier Based on Graphene

Authors:
Samuel Ver Hoye
Andreea Hadarig
Carlos Vázquez
Miguel Fernández
Leticia Alonso
Fernando Las Heras

Affiliation: Signal Theory and Communications Group, Department of Electrical Engineering, University of Oviedo, 33203 Gijón, Spain

Abstract: The research paper presents a single stage millimeter/submillimeter wave frequency multiplier based on graphene. It focuses on the nonlinear excitation of high-order harmonic components in a graphene film when illuminated with a low-frequency input signal. The document discusses the design, optimization, and experimental characterization of the frequency multiplier, emphasizing its performance and potential applications. The study aims to address the technological bottleneck in signal generation at submillimeter wave and lower THz frequency bands, with potential applications in various fields such as security, medical diagnostics, and nondestructive testing.

Publication Details:
Received January 18, 2019
Accepted February 17, 2019
Date of Publication February 25, 2019
Date of Current Version March 12, 2019
Digital Object Identifier 10.1109/ACCESS.2019.2901577

# 2. harpret 2313837
Title: Mixed Precision Training
Author(s): Sharan Narang, Gregory Diamos, Erich Elsen
Affiliation(s): Baidu Research
Abstract: This paper presents a methodology for training deep neural networks using IEEE half-precision format (FP16), which can significantly reduce memory requirements and speed up arithmetic on recent GPUs without sacrificing model accuracy. The document introduces three techniques to prevent the loss of critical information when using half-precision format: loss-scaling, master weight copies, and FP32 master copies. These techniques help maintain model accuracy while benefiting from the computational advantages of reduced precision arithmetic.
Publication Details: Published as a conference paper at ICLR 2018 [T2], [T3].

# 3. mandeep 2316755
Title: On-Device Neural Net Inference with Mobile GPUs
Authors: Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pisarchyk, Mogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei Kulik, and Matthias Grundmann
Affiliation: Google Research, 1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA
Abstract: The abstract discusses the benefits of on-device machine learning, focusing on improved inference latency, reduced server dependency, and enhanced privacy for users. It highlights the use of mobile GPUs for real-time inference on mobile phones, emphasizing the importance of designing networks that are mobile GPU-friendly. The paper also introduces a state-of-the-art mobile GPU inference engine integrated into TensorFlow Lite.
Publication Details: The paper was published by Google Research and is available at https://tensorflow.org/lite.

# 4. gurkirat 2317089
Title: Massively Parallel Methods for Deep Reinforcement Learning
Author(s): Alex Krizhevsky, Volodymyr Mnih, David Silver
Affiliation(s): Google DeepMind, University of Toronto
Abstract: This paper introduces a distributed architecture for deep reinforcement learning that combines the power of deep neural networks with the scalability of distributed systems. The proposed method achieves state-of-the-art results on a range of Atari 2600 games, demonstrating the effectiveness of parallelization in deep reinforcement learning.
Publication Details: Presented at the 30th International Conference on Machine Learning, 2013

# 5. saruar 2322617
Title: Horovod fast and easy distributed deep learning in TensorFlow

Authors:
Alexander Sergeev, Uber Technologies, Inc., asergeev@uber.com
Mike Del Balso, Uber Technologies, Inc., mdb@uber.com
Affiliation:
Uber Technologies, Inc.

Abstract:
The document discusses the challenges and solutions related to distributed training of machine learning models using TensorFlow. It introduces Horovod, an open-source library developed by Uber, which aims to simplify and speed up distributed deep learning projects with TensorFlow. The document outlines the issues with standard distributed TensorFlow and describes how Horovod addresses these challenges by improving inter-GPU communication and requiring minimal modifications to user code. It also introduces the concept of Tensor Fusion, an algorithm that fuses tensors before performing ring-allreduce, resulting in significant performance improvements. Additionally, the document highlights the Horovod Timeline, a profiling tool for understanding the states of worker nodes during a distributed training job. The overall focus is on making distributed training more efficient, easier to implement, and faster.

Publication Details:
Title Horovod fast and easy distributed deep learning in TensorFlow
Author Alexander Sergeev, Mike Del Balso
Affiliation Uber Technologies, Inc.
Email asergeev@uber.com, mdb@uber.com
Published in arXiv1802.05799v3 [cs.LG]
Publication Date 21 Feb 2018
